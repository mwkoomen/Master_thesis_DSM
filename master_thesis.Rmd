---
title: "Mitigating panel attrition with synthetic data"
subtitle: "Statistical disclosure control for linked panel survey and register data"
author:
- "Maarten Koomen"
- "Master thesis in Applied Data Science & Measurement (Mannheim Business School)"
- "Supervisor/Examinator: Jörg Drechsler & Stefan Bender"
- "Link to [Github repository](https://github.com/mwkoomen/Master_thesis_DSM) with R code."
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    highlight: espresso
    toc: no
    number_sections: true
header-includes:
    - \usepackage{setspace}\doublespacing
bibliography: references.bib
link-citations: yes
linkcolor: blue
---
\thispagestyle{empty}
\newpage
```{=latex}
\setcounter{tocdepth}{4}
\tableofcontents
```
\newpage
```{r libraries, include=F}
library(rio)
library(tidyverse)
library(synthpop)
library(expss)
library(DescTools)
library(nnet)
library(rmarkdown)
library(gghighlight)
library(sjlabelled)
library(gridExtra)
library(tidytext)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
knitr::opts_chunk$set(fig.align = 'left')
```

```{r load data, include=F}
rm(list=ls())
source("H:/IPSDS/Master thesis/Data/data_tree_ext.R") # Only runs within Uni Bern intranet
obs <- import.tree()
for (v in c(1,2,5,7,8,13,14,15)){
  obs[v] <- as.factor(obs[,v])
}
```

# Introduction
Attrition can be a huge problem for panel studies. Selectivity in who chooses to participate in a panel survey diminishes the underlying representativeness of the survey data and, by extension, the general validity of statistical inferences. By combining panel survey and register data it is generally possible to fill in some of the gaps created by the explicit or implicit (e.g. nonresponse) refusal to participate in panel waves. \par

Publishing such linked data would be a serious infringement on the respondents' privacy and a violation of most conventional data protection regulations. However, the generation of synthetic data based on linked data might offer an opportunity to make such sensitive information accessible for research without infringing on the respondents' privacy. In this paper, I will assess the feasibility of generating synthetic data from the Swiss education panel (TREE) linked with registry spell data on educational enrollment (LABB) from the Swiss Federal Statistics Office (FSO). Merging these two data sets combines the feature richness of survey data with the (theoretical) full coverage of the target population in the registry data. The goal is to combine these two data sources and generate synthetic data which are analytically comparable to the observed data without an unreasonable increase in the likelihood of a breach of data privacy when disseminated to the public. \par

I will start by giving a short overview of the history and practical applications of data synthesis as a method of statistical disclosure control for the dissemination of micro-data. Next, I will discuss some metrics commonly used to measure the data utility and disclosure risk of synthetic data, and select which of these I will use to fine-tune and asses the performance of the final synthesis model. Third, I will briefly describe the data collection process and main characteristics of both sets of observed data; the TREE panel study and the LABB registry data. Finally, I will compare different synthesis configurations and use the data utility and disclosure risk metrics discussed in section two to judge the performance of the final synthesis model. \par

# Statistical disclosure control with synthetic data 
Releasing micro-data to the public carries with it a, usually unknown, risk of information disclosure. Such a risk could be that an attacker (i.e. someone intended on disclosing information) can determine that a specific individual was in the original sample or that specific information can be linked to individuals with high probability. Increasingly inexpensive computing power has led to a higher demand by researchers for direct access to micro-data. This has created a need for statistical agencies and other data producers to find solutions to data dissemination that limit the potential for privacy breaches. Not all such solutions focus on releasing micro-data to the public. Data centers can also set up on-site or online infrastructures where researchers are allowed to access the sensitive micro-data. The researchers are typically allowed to analyze the sensitve data on-site and take with them only the aggregated results after they are checked for potential confidentiality issues. This method is flexible but can be rather burdensome on both the data center and the researchers. In case of a physical on-site solution, the data center needs to provide isolated machines for the researchers' and the researchers output have to be checked. In addition, the researchers have to physically travel to the data center, making any later discovered mistakes or additional need for analysis fairly costly. Online infrastructures give easier access to researchers and are potentially less costly for institutions, however, it can be difficult to define the amount of access allowed to researchers and how this impacts on the risk of disclosure. \par 

An alternative to these methods is releasing scientific-use-files that have been modified in some way by the data disseminator. Methods of modification depend on the type of data but they have broadly focused on methods of information reduction, where information that poses a disclosure risk is suppressed in some way (e.g. top coding, rounding, and global recoding), and methods of data perturbation, where values are generally altered to improve data confidentiality (e.g. value swapping, noise addition, and data synthesis). A downside of many of these methods is that they often require specific knowledge of the modification process to properly analyze the altered data. Synthetic data as a method of statistical disclosure control offer a promising alternative because the data can be analysed by using relatively straightforward methods. \par

## Synthetic data: synopsis of methodology and practical applications
The idea of data synthesis as an approach to statistical disclosure control was first proposed by Rubin [-@rubin1993] and Little [-@little1993]. These early contributions focused on using multiple imputation techniques to generate multiple synthetic records for non-observed units or replacing sensitive records with synthetic ones. In the computer science literature, the idea can be traced back to Liew et al. [-@liew1985] who discuss using a very similar approach as a data distortion method. After these initial proposals, it took another ten years before the methodology was fully formalized. There are different approaches to generating synthetic data but the general idea is that models are fitted to the original, observed data, then, random draws from the fitted models are used to replace the ordinal data. Broadly speaking, there are two approaches; full- and partial data synthesis. As the name suggests, in fully synthetic data, all records and values are fully synthetic. In partial synthetic data, only some sensitive records and/or values are replaced with synthetic data, while leaving all other records and values untouched. In theory, fully synthetic data should offer better privacy protection because it contains no (directly) observed information. The flip-side is that, compared to partial synthesis, the analytical validity of fully synthetic data depends more heavily on the synthesis models being specified correctly. \par

Raghunathan et al. [-@raghunathan2003] and Reiter [-@reiter2003] formulated the rules for achieving valid inferences from fully-, and partially synthetic data, respectively. These methods are a variant on the combining rules for multiple imputation techniques for nonresponse, differing slightly for full- and partial synthesis. Raab et al. [-@raab2016] further expanded these combining rules by adding a method to achieve valid inferences in cases where only one synthetic data set is generated. This can be helpful in cases where the data synthesis is computationally intensive (for example when synthesizing data for large samples of observed data) or if data publishers fear that releasing multiple synthetic data sets generates an unacceptable additive risk of disclosure. \par

In the social sciences, early practical adoptions of synthetic data have focused primarily on fitting parametric models to generate synthetic data. Over the years, these methods have been expanded by including modelling approaches based on machine learning algorithms and models that can account for the complex sampling structures of modern survey data. Machine learning (ML) approaches are especially promising in situations where higher order relationships between a large number of variables need to be modeled in the synthesis process. For example, parametric models that have dozens of categorical predictors might not converge and suffer from issues of multi-collinearity or perfect prediction. ML approaches can be helpful in these cases as they are not affected by these problems and offer an automated way of modelling any relevant higher order relationships in the original data. A suit of different ML techniques have been tested as synthesizers over the years; Classification and Regression Trees (CART) [Reiter -@reiter2005], Random Forest [Caiola and Reiter -@caiola2010], Support Vector Machines [Drechsler -@drechsler2010b], and Generative Adversarial Networks (GANs) [Choi et al. -@choi2017; Park et al. -@park2018]. In comparing some of these ML methods, Drechsler and Reiter [-@drechsler2011] and Little et al. [-@little2021] show that CART models generally outperform some of the newer ML algorithms when used as a data synthesizer. \par

In addition to selecting a parametric or ML data synthesizer, another choice in the data synthesis process is the use of either sequential or joint modelling. Joint modelling aims to directly specify the joint distribution of the original data and generate synthetic data by drawing values from this joint distribution [see Schafer -@schafer1997 for a detailed discussion on joint modelling for the mulitvariate normal- and log-linear model]. Valid synthetic data can be easily generated if the joint distribution of the original data is correctly specified. However, given the complexity of real-world data, it is often difficult to correctly identify the joint distribution. This is especially true if the data consist of both continuous and categorical variables, a common characteristic of social science micro-data. An alternative to joint modelling is synthesizing variables in sequence where each variable is synthesized by using as predictors only those variables that have already been synthesized previously, plus, in the case of partial synthesis, any variables that remain unchanged in the final data set. The assumption of sequential modelling is that the underlying joint distribution of a particular set of variables can be represented by the product of their conditional univariate distributions. This approach is very flexible since it allows each variable to be modeled separately, given the option of using either parametric or ML methods. \par

The earliest real-world application of synthetic data is from 1997, when the U.S. Federal Reserve Board decided to synthesize certain information at high risk of disclosure in the Survey of Consumer Finances [Kennickell -@kennickell1997]. An early example of the usefulness of synthetic data in the disclosure control of linked data is given by Abowd and Woodcock [-@abowd2001], who generate a synthetic data based on data from the French National Institute of Statistics and Economic Studies (INSEE). Their goal was primarily to reduce the disclosure risks when linking data from several different official registers. To date, the most complex and extensive linked synthetic data has been released by the U.S. Census Bureau (Abowd et al. [-@abowd2006]; Benedetto et al. [-@benedetto2018]). This data contains synthesized records based on linked data from the Survey of Income Program Participation (SIPP), the Social Security Administration, and the Internal Revenue Service. This longitudinal data contains over 600 variables, almost all of which have been synthesized. In Europe, several data centers have started to produce similar statistical products. The German Institute for Employment Research (IAB) released a partial synthetic data in 2011 that was based on one wave of its Establishment Panel [Drechsler -@drechsler2012]. The Scottish Longitudinal Study (SLS) has used a tailor-made synthetic data approach to grant access to census data linked with sensitive records from health and death registers (Nowok et al. [-@nowok2017]). In 2015, Eurostat published a synthetic version of the EU Statistics on Income and Living Conditions (EU-SILC) [de Wolf -@deWolf2015]. The synthetic EU-SILC data are not designed to lead to valid inferences, instead the data primarily facilitate easy access to otherwise sensitive micro-data for researchers to explore and prepare their analysis whilst they request full access to the original data. \par 

## Assesing the quality of synthetic data 
Generating and publishing synthetic data is only useful if both data disseminator and the research community can be equally convinced of its utility as a proxy of the original data, and of the effectiveness of the synthesis process in reducing the risk of disclosure. These two characteristics of synthetic data are logically opposed, i.e. synthetic data that can in no way be discriminated from its original base would offer zero additional protection against re-identification of individuals or the disclosure of sensitive information. A broad range of formal tests for both the utility and disclosure risk of synthetic data have been proposed. In the following two subsections, I will introduce some of the most prevalent of these metrics and select three that I will use in the remainder of the paper. \par

### Measuring data utility 
Data utility measures can roughly be grouped into three broad categories. First, synthetic data is usually checked on its general consistency and distribution. These checks can be labelled as fit-for-purpose measurements, they are aimed towards comparing marginal or conditional distributions and checking whether values in the synthetic records are in themselves plausible (e.g. no negative number of children) and conditionally plausible (e.g. no unemployment with full-time job). These checks are generally not designed to illustrate the utility of the data; synthetic data with a one-to-one marginal distribution to the original data might still lead to invalid inferences in more complex multivariate analyses. However, they are a first check to see if there are underlying issues with the synthesis models. For this paper, I will limit these checks to comparing the marginal distributions of the orginial and synthetic variables and checking for implausible values in the synthetic data. \par

A second approach, often classified as global or general utility metrics, aims to test the utility of synthetic data by making general, formal, comparisons between the synthetic and original data. Many global measures utilize some distance metric to compare the synthetic and original data, such as the Kullback-Lieber divergence [Karr et al. -@karr2006] or the Hellinger distance [Gomatam and Karr -@gomatam2003]. Another technique that has gained in popularity is based on the literature of propensity score matching [Rosenbaum and Rubin -@rosenbaum1983]. In this approach, the synthetic and original data are stacked into a single file, then, the probability for each row being a synthetic row is calculated. The closer the synthetic data are to the original data, the harder it would be for the propensity model to distinguish between synthetic and original records. Several metrics can be used to evaluate the difference between propensity scores, such as the Kolmogorov-Smirnov distance [Bowen et al. -@bowen2021] or the propensity score mean squared error ($pMSE$) [Woo et al. -@woo2009; Snoke et al. -@snoke2017]. The $pMSE$ has become a particular popular global utility measure in recent years, it is defined as: \par    

\hfil \begin{equation}pMSE=\frac{1}{N}\sum_{i=1}^{N}[\hat{p}_{i}-c]^2 \end{equation} \par

Where $\hat{p_i}$ is the estimated propensity for record $i$ in the stacked synthetic and original data ($N = n_{org} + n_{syn}$) to be from $n_{syn}$, with $c=n_{syn}/N$. Smaller $pMSE$ values indicate higher analytical validity for the synthetic data as it is harder for a propensity model to distinguish between observed and synthetic records in the stacked data set. A downside to this approach is that the $pMSE$ generally increases with the number of predictors in the synthesis model. To overcome this, @snoke2017 define a standardized $pMSE$ by subtracting the $pMSE$ by its expectation under a null model (a correctly specified synthesis) divided by the its standard deviation. The null $pMSE$ is distributed as a multiple of a chi-squared distribution with $(k-1)$ degrees of freedom and expectation and standard deviation given by: \par 

\hfil \begin{equation}E[pMSE]=(k-1)(\frac{n_{org}}{N})^2(\frac{n_{syn}}{N})/N=(k-1)(1-c)^2c/N\end{equation} \par

\hfil \begin{equation}StDev(pMSE)=\sqrt{2(k-1)}(\frac{n_{org}}{N})^2(\frac{n_{syn}}{N})/N=\sqrt{2(k-1)}(1-c)^2c/N\end{equation} \par

As a global utility measure, the standardized propensity score mean squared error ($S\_pMSE$) offers a clear interpretation and straightforward comparison between different synthesis models. For the remainder of the paper, I will use the $S\_pMSE$ to fine-tune the synthesis parameters in Secion 4. \par

Global utility measures benefit from not having to know a priori how synthetic data will be analyzed. However, there is no guarantee that synthetic data with a high global utility are in fact suitable for any one specific analysis. A third and last class of measurements therefore focuses on outcome-specific utility metrics, where specific analyses on the synthetic and original data are run and compared in parallel. Point estimates (means, regression coefficients) obtained from the synthetic and original data can be plotted against each other. If the synthetic data have a high outcome utility, these point estimates should cluster around a diagonal with a gradient of one. However, this approach does not account for any differences caused by the inherent uncertainty of estimation. In some situations, deviating point estimates obtained from the synthetic and original data could simply be an artifact of a large sampling error. A widely used metric that can take this estimation uncertainty into account is based on the overlap of confidence intervals [Karr et al. -@karr2006]. The CI overlap $J$ is defined as the average relative overlap for point estimate $k$ as: \par

\hfil \begin{equation}J_{k}=\frac{1}{2}[\frac{U_{over,k}-L_{over,k}}{U_{org,k}-L_{org,k}}+\frac{U_{over,k}-L_{over,k}}{U_{syn,k}-L_{syn,k}}]\end{equation} \par

Where $U_{org}$, $L_{org}$ and $U_{syn}$, $L_{syn}$ are the upper- and lower bounds of the CI on the original and the synthetic data, respectively, and $U_{over}$ and $L_{over}$ are the upper- and lower bounds of the overlapping sections of the CI estimates on the original and synthetic. In Section 4.3, I will use this method to asses the specific utility of the generated synthetic data sets. \par    

### Measuring disclosure risk
There is a general distinction between measuring the disclosure risk of full- or partial synthetic data. With partial synthesis, at least some records or values remain unchanged and there is usually a one-to-one relation between the synthetic and original data. In the case of full data synthesis, this one-to-one relation does not exist and the original and synthetic data can be of different sizes. However, even though no original information remains in a fully synthetic data set, this does not mean that it cannot be used to learn about and disclose information contained in the original data. For example, Stadler et al. [-@stadler2022] show how prior knowledge about the true values of some target records can be used in combination with information on the synthesis process to determine whether specific units are contained in the original data. Another approach proposed by Taub and Elliot [-@taub2019] uses a measure that directly matches cases in the original and synthetic data based on a defined set of key variables, and then calculates the disclosure risk of specific information in target variables. Specifically, they calculate a *Targeted Correct Attribution Probability* (TCAP) by finding records in the synthetic data that have a low variability on a vector of *target* variables within an equivalent class defined by a vector of *key* variables and then calculating the probability that those records can yield the true value of the *target* variables in the original (linked) data. Let $d_o$ be the original data and $K_o$ and $T_o$ vectors for the key and target variables so that: \par  

\hfil \begin{equation} d_o=\{K_o,T_o\} \end{equation} \par

With the synthetic data $d_s$ being: \par

\hfil \begin{equation} d_s=\{K_s,T_s\} \end{equation} \par

First, the *Within Equivalence Class Attribution Probability* (WEAP) is calculated: \par

\hfil \begin{equation}WEAP_{s,j}=Pr(T_{s,j}|K_{s,j})=\frac{\sum_{i=1}^{n}[T_{s,i}=T_{s,j},K_{s,i}=K_{s,j}]}{\sum_{i=1}^{n}[K_{s,i}=K_{s,j}]}\end{equation} \par

The WEAP score for record $j$ is the probability of the target variables conditional on the key variables. The square brackets are Iverson brackets (1 if condition is true, 0 otherwise) and $n$ is the number of records in the synthetic data. The aim is to retain records with a high WEAP score since those synthetic records would be most helpful in trying to guess at the true values of the target variables in the original data. For rows with a high WEAP score (for example WEAP=1), the TCAP for record $j$ in the synthetic data is given by: \par

\hfil \begin{equation}TCAP_{s,j}=Pr(T_{s,j}|K_{s,j})_{o}=\frac{\sum_{i=1}^{n}[T_{o,i}=T_{s,j},K_{o,i}=K_{s,j}]}{\sum_{i=1}^{n}[K_{o,i}=K_{s,j}]}\end{equation} \par

For synthetic records $j$ that have no counterpart in the original data $d_o$ matched on the key variables, the denominator in Equation 8 would be zero and the TCAP would be undefined. For records that do match, the TCAP  score lies somewhere between 0 and 1, where a score of 0 would mean there is no disclosure risk for these records and a value of 1 would indicate that, for records with low *l*-diversity, there is a considerable risk to disclose the true values of the target variables if the synthetic data were to be made public. The TCAP measure seems highly appropriate for assessing the added disclosure risk of linked data and I will use it in Section 4.4 to assess the disclosure risk of the synthesized data. \par 

# Observed data
The main goal of this paper is to specify a synthesis model that can be used to generate scientific-use-files from data that would otherwise be too sensitive to be published. Specifically, I am interested in linking survey data from the Swiss education panel (TREE) with register data on educational enrollment (LABB) from the Swiss Federal Statistical Office. The main idea is that this approach could facilitate the creation of a data set with the full TREE baseline sample of +20'000 participants who can be followed throughout their educational careers. This merger would reap the benefits of having data that is rich in features (TREE) and data that has a high degree of coverage (LABB) of the target population. In the next two subsections, I will briefly describe some important design features of both data sources. \par

## The TREE and LABB data
The Transitions from Education to Employment (TREE) panel is a multi-cohort, multidisciplinary longitudinal large scale survey providing high-quality data on educational and occupational pathways in Switzerland. The target population are school leavers who are first surveyed at the end of compulsory school at the age of approximately 15 to 16 years (see @tree2021 for more details). For the purpose of this paper, I will focus only on the second TREE cohort who left compulsory school and were first surveyed in 2016. The baseline survey wave for the second TREE cohort contains detailed student and parental background characteristics and measurements of cognitive skills. The TREE survey was part of the Assessment of the Attainment of Educational Standards (AES), a national monitoring scheme designed to capture student skills in mathematics at the end of lower-secondary education in Switzerland. In 2016 the AES was designed as a compulsory, cross-sectional in-school assessment, carried out under the responsibility of the Swiss Conference of Cantonal Ministers of Education (EDK/CDIP). Because it was compulsory, the response rate of the baseline survey was close to a hundred percent. The TREE survey gathered explicit consent from the AES respondents to link their data with any further collected longitudinal information (with an overwhelming majority (95%) giving their approval). Subsequent TREE survey waves continue to collect data on education and labor market trajectories, contextualized by a rich set of complementary information. An obvious downside of these subsequent waves is that participation is fully voluntary, response rates for the first two baseline surveys where around 85 and 75 percent, respectively. \par

The *Längsschnittanalysen im Bildungsbereich* (LABB) data are micro-data on education trajectories. Started in 2014, the LABB combines data from several FSO registers, primarily from several education monitors (*Statistik der Lernenden* (SdL), *Statistiken der Abschlüsse* (SBA), *Statistik der beruflichen Grundbildung* (SBG), and the *Schweizerisches Hochschulinformationssystem* (SHIS)), in addition to data from the population register (*Statistik der Bevölkerung und der Haushalte* (STATPOP)) and the Swiss micro-census (*Strukturerhebung* (SE)). Similar to the second TREE cohort data, since 2016, the LABB micro-data can be used to analyse transitions into post-compulsory, upper-secondary education. Unlike the TREE data, however, the LABB micro-data is limited to educational programs that last at least one full-time semester. It does not contain information on some intermediate educational options in the post-compulsory transition period like the Motivation Semester or the Au-pair or Foreign Language Stays (for more detail see: FSO [-@bfs2021;-@bfs2022]. For all remaining formal educational programs, the LABB data provide population level data that does not suffer from non-response biases that usually plagues voluntary based surveys like TREE. \par  

```{r bias, eval=F}
cat <- c("No / other education",
        "VET (2-3 yrs)",
        "VET (4 yrs)",
        "Specialised school",
        "Baccalaureate",
        "Survey nonresponse")
d <- c(rep("TREE panel data [weighted]",6),
       rep("LABB register data",6)
       )
 nbias <- data.frame(
   Education = factor(c(rep(cat,2)),
                      levels = c(
                        "Survey nonresponse",
                        "No / other education",
                        "Specialised school",
                        "VET (2-3 yrs)",
                        "VET (4 yrs)",
                        "Baccalaureate"
                      )),
   Data = factor(d,levels = c(
     "LABB register data",
     "TREE panel data [weighted]"     
     )
  ),
   Value = c(
     16.3,29.3,13.6,5.3,30,5.4,
     21.8,32.9,14.7,5.2,25.5,0
  )
 )
b <- ggplot(nbias, aes(x=Education,y=Value, color=Data)) + 
  geom_col(position = position_dodge(0.7),fill = NA,size = 2,width=0.5) + 
  coord_flip() +
  xlab("") +
  ylab("Percent of sample/population") + 
  theme(legend.position="top",
        legend.title = element_blank(),
        axis.title = element_text(size=10)) + 
  scale_color_manual(values = c("LABB register data"="blue", "TREE panel data [weighted]"="red"))
  #save
    png(file="H:/IPSDS/Master thesis/Pictures/bias.png",width=800, height=600, res=150)
    b
    dev.off() 
```

```{r bias-fig, out.width="75%", fig.cap="2017 eduction enrollment of 2016 school-leaving cohort."}
knitr::include_graphics("H:/IPSDS/Master thesis/Pictures/bias.png")
```

Figure 1 shows the TREE response bias and how the LABB data might fill this gap by plotting the upper-secondary enrollment percentages for the 2016 compulsory school-leaving cohort. The TREE figures are adjusted by their sampling design weights to represent population averages. In 2017, the first TREE survey wave had around five percent survey nonresponse. Among those that did respond to the survey, students that enroll into a Baccalaureate type of upper secondary education (high-school, college, etc.) are over-represented. The LABB figures represent the true population enrollment distribution. In comparing both data sources, respondents in the TREE panel are biased towards students in higher academic tracks and underrepresented by students in vocational- or other educational tracks. The LABB register data can be used to understand this bias and, ideally, to fill in some of the gaps in the TREE data. However, it would be unethical to make data publicly available that directly ignores the explicit or implicit refusal to panel participation. The aim of this paper is therefore to see if it would be feasible, both from a data validity and privacy standpoint, to create a set of synthetic data that could be used as a publicly available scientific-use-file. \par 

## Variable selection: who accesses secondary education in Switzerland? 
The TREE panel scientific-use-files contain hundreds or even thousands of variables. Synthesizing all of these is firmly beyond the scope of this paper. As a feasibility study, I will limit myself to synthesizing only a handful of variables that model the transition from lower-secondary to post-compulsory education in Switzerland. The Swiss educational landscape is highly federalized with each canton having a high level of autonomy in setting educational policy. The Swiss system is characterized by early tracking with relative high levels of differentiation and stratification. Tracking generally starts in lower-secondary education around age 12 in one of five distinctive tracks based on academic requirements (basic, intermediate, or advanced academic requirements, plus two options with integrated or no tracking on academic ability) [SKBF -@skbf2018]. Placing guidelines for lower-secondary tracks vary between cantons but they are most commonly based on prior scholastic performance measured by grades [Neuenschwander et al. -@neuenschwander2012]. The main differentiation at the upper-secondary level, is between vocational education and training (VET) and general education, mainly typified by baccalaureate type schools that grant direct access to Swiss universities. For both types of upper-secondary education, admission to the more academically demanding tracks is mainly based on prior school track and performance measured by grades [Buchmann et al. -@buchmann2016]. In French and Italian speaking cantons there is generally a higher share of youths that enroll for general (non-VET) upper-secondary education compared to the German-speaking regions [SKBF -@skbf2018]. Scientific research into determinants of upper-secondary track placement have generally confirmed the importance of grades and early tracking [Baeriswyl et al. -@baeriswyl2006; Neuenschwander and Malti -@neuenschwander2009; Beck -@beck2014]. Compared to boys, girls are found to be slightly over-represented in  tracks with higher academic requirements, mostly because of better performances measured by grades, but also because they have higher achievement motivation [Glauser -@glauser2015]. Having a migration background increases the likelihood of attending a track with lower academic requirements, although there is considerable variation between different migrant origin groups [Beck -@beck2014; Glauser -@glauser2015]. Another common research finding is that there is a persistent influence of social background (e.g. parental socio-economic status or educational attainment) on upper-secondary track placement [Imdorf -@imdorf2005; Neuenschwander and Malti -@neuenschwander2009;Hupka-Brunner et al. -@hupka2010; Falter and Wendelspiess -@falter2011; Falter -@falter2012; Combet -@combet2013; Glauser -@glauser2015]. \par

In light of these research findings, Table 1 lists 15 variables selected for data synthesis from the linked TREE & LABB data. First, personal background information on gender, language region, and immigration status is included. In the unweighted TREE data there is a slight bias towards female participation (54.5% females versus 45.5% males). The language region is coded as binary with the Italian language region being grouped together with French-speaking regions because of their low case counts (69.8% and 30.2% for German- and French/Italian language regions, respectively). Immigration status is a categorical variable with three levels; native Swiss (72.7%), second-generation immigrants (18.7%),and first-generation immigrants (9.3%). Second, I include information on the socio-economic background of respondents in the form of their parents' occupation, educational attainment, reading interest, educational aspirations (for the respondent), family affluence, and family wealth. Parental  occupation is measured by the highest parental ISEI-08 code [Ganzeboom -@ganzeboom2010]. Parental education is measured by a variable with two categories; parents with secondary education or lower (62.3%), and parents with at least one completed tertiary education (40.7%). Reading interest is measured by a composite variable based on the reading interest of both parents [Sacchi and Krebs-Oesch -@sacchi2021]. The parental aspirations for the respondents' educational careers is measured by a categorical variable with four levels; tertiary (34.5%), vocational education (50.6%), compulsory school (1.4%), and no opinion (13.5%). Family affluence and wealth are both composite variables scaled on household possessions and spending patterns (for more details see @kunter2002; @hartley2016; Hobza et al. [-@hobza2017]. On early tracking, ability, and performance, I include the attended type of lower-secondary school, average school grade in the regional language, maths, and science classes, and the score on the compulsory standardized maths test (AES) taken in 2016. Lower-secondary school type is measured by the level of academic requirements for the attended school with four categories; high requirements (29.1%), advanced requirements (39.1%), basic or low requirements (29.7%), and no or alternative differentiation based on skill level (2.1%). The average school grade is the mean of grades in maths, natural science courses, and marks in the language of AES test (e.g. mostly equivalent to primary regional language, either German, French, or Italian). The maths test score is a weighted likelihood estimate based on the individual AES maths test items (for details on test design and item scaling see @domenico2019). Lastly, respondents' idealistic educational aspiration and their embodied cultural capital are added. The idealistic educational aspiration is measured as a categorical variable with three levels; compulsory education (0.5%), upper-secondary (42.5%), or tertiary (57%). Embodied cultural capital is a composite variable scaled on questions on behavioral and verbal skills (for more details see @hupka2016; @sacchi2021). \par    

\textbf{Table 1: Selected synthesis variables}

\setstretch{1}

| \#| Variable| Year | Description           | Values         | 
|:-|:----|:-:|:------------------------------|:------------------|
| 1   | sex     | 2016 | Gender of respondent                            | \scriptsize 1=Female, 2=Male | 
| 2   | langreg | 2016 | Language region                                 | \scriptsize 1=German, 2=French/Italian | 
| 3   | wlem    | 2016 | Maths test: weighted likelihood estimates (WLE) | \scriptsize \emph{Continuous} | 
| 4   | marks   | 2016 | Mean school marks (test-language/maths/science) | \scriptsize \emph{Continuous} | 
| 5   | ls_req  | 2016 | Lower-secondary school requirements             | \scriptsize 1=High, 2=Advanced, 3=Basic/Low, 4=No/non-assignable | 
| 6   | hisei08 | 2016 | Highest parental occupational code (ISEI 08)    | \scriptsize \emph{Continuous} | 
| 7   | pareduc | 2016 | Parents' highest educational attainment         | \scriptsize 0=Secondary or less, 1=Tertiary | 
| 8   | immig   | 2016 | Immigration status                              | \scriptsize 1=Native, 2=Second generation, 3=First generation | 
| 9   | wealth  | 2016 | Household possessions: family wealth            | \scriptsize \emph{Continuous} | 
| 10  | joyreadp| 2016 | Parental reading interest                       | \scriptsize \emph{Continuous} | 
| 11  | fas     | 2016 | Family affluence scale                          | \scriptsize \emph{Continuous} | 
| 12  | inccap  | 2016 | Embodied cultural capital                       | \scriptsize \emph{Continuous} | 
| 13  | aspmf   | 2016 | Parents' educational aspirations                | \scriptsize 1=Tertiary, 2=VET, \newline 3=Compulsory school, 4=No opinion | 
| 14  | aspideal| 2016 | Student's idealistic educational aspirations    | \scriptsize 0=Compulsory school, 1=Upper-secondary, 2=Tertiary | 
| 15  |us_enroll| 2017 | Upper-secondary: education enrollment           | \scriptsize 1=No/other education, \newline 2=Bridge programme, \newline 3=Upper-sec VET, \newline 4=Upper-sec VET (Bac), \newline 5=Upper-sec Academic Bac. | 
\setstretch{2}
Besides forming a theoretically cohesive set, the variables in Table 1 are also selected because they do not contain any dependencies that might be difficult to reproduce in the data synthesis process. In principle, for these specific variables, there is no configuration of values which is inherently impossible. This will make the data synthesis process much more straightforward because there is no need to explicitly model dependencies between sets of variables. For example, if age and marital status would have been included, the data synthesis should take into account that no person under 18 could be legally married under Swiss law. In addition, missing values that are present in the observed data, will not be replaced before data synthesis, meaning that they will effectively count as extra categories and will be present in the synthetic data  sets. This approach is useful if the goal of the synthetic data is to give researchers easy access to data that preserves the structure of the observed data as closely as possible, including information on missing patterns present in the original data. \par 

# Data synthesis
For the generation of the linked TREE and LABB synthetic data, I will solely focus on fitting CART and parametric models, ignoring some of the other ML approaches available. Further, I will only explore sequential synthesis, i.e. synthesizing each variable in sequence. An alternative to this approach would be to model the joined distribution of the entire data, something that is often prohibitive with real world survey data with even a handful of variables. In the next two sections, I will explore the synthesis sequencing and the modelling of each individual variable to find parameters that generate synthetic data with high utility. For the data synthesis and most model evaluations, I will be using the R package *synthpop* [Nowok et al. -@nowok2016]. This package was created as part of the SYLLS (Synthetic Data Estimation for UK Longitudinal Studies) project. The goal was to create a toolkit that can be used to quickly create bespoke synthetic data from sensitive micro-data that fits the particular needs of individual researchers. The package offers a range of tools that are useful for both the creation and the analysis of synthetic data. It should be noted that the initial idea behind the *synthpop* package is that the synthetic data generated are meant to be used as test data for researchers to explore and test their models on. Code developed on synthetic data should ultimately be run on the original data to verify results. The *synthpop* package is flexible, supporting the use of a range of parametric models and ML algorithms. In addition, the *synthpop* packages can handle missing data in categorical and continuous variables. For missing values in categorical variables this process is relatively straightforward since they are simply regarded as separate categories in the imputation process. For continuous variables this process is a little more involved. First, an auxiliary categorical variable is created containing all parallel missing categories. Values in this auxiliary variable are synthesized by either a polytomous or CART model. The remaining, non-missing, values are fitted separately and then both these variables are used in the final synthesis and as a predictor for the synthesis of remaining variables. \par 

## Sequencing
A first step in finding a well-performing synthesis model for our observed data is to consider the order in which our variables are synthesized, which can have a significant impact on the utility of the synthetic data. A brute force method for finding the optimal sequence of data synthesis is computationally prohibitive for data sets with even a handful of variables. For $n$ variables a total of $n!$ synthesis sequences have to be compared. According to @drechsler2011, it can be computationally beneficial to place categorical variables at the end of the synthesis sequence, especially if they have many categories. Sub-setting numerical and categorical variables in the synthesis sequencing can significantly reduce the search grid for a brute force approach. In addition, variables that follow a normal or binomial distribution would be easier to synthesize earlier in the sequence because, even with few predictors in the synthesis model, fairly accurate synthesis can be achieved by simply taking random draws from the normal or binomial distribution. \par

```{r qqplot, eval=F}
png(file="H:/IPSDS/Master thesis/Pictures/qq.png",width=900, height=400, res=150)
par(mfrow=c(1,3))
qqnorm(obs$wlem, pch = 1, frame = FALSE, main = "Maths test (wlem)",xlab="",cex.main=0.9)
qqline(obs$wlem, col = "steelblue", lwd = 2)
qqnorm(obs$marks, pch = 1, frame = FALSE, main = "Average grades (marks)", ylab="",cex.main=0.9)
qqline(obs$marks, col = "steelblue", lwd = 2)
qqnorm(obs$inccap, pch = 1, frame = FALSE, main = "Cultural capital (inccap)", xlab="",ylab="",cex.main=0.9)
qqline(obs$inccap, col = "steelblue", lwd = 2)
dev.off() 
```

```{r qqfig,out.height="60%",fig.align="center",fig.cap="Normal Q-Q Plot."}
knitr::include_graphics("H:/IPSDS/Master thesis/Pictures/qq.png")
```

All numeric variables in the observed data are composite variables that are based on sets of underlying categorical variables. In general, these variables are approximately normal distributed with some degree of variability. The variables *wlem*, *marks*, and *inccap* have distributions that are closest to normal. Figure 2 shows the normal Q-Q plots for these variables. Of these three, *wlem* is closest to having a normal distribution. The variables *marks* and *inccap* are only quasi-normal in that their distributions are clustered around certain values or because the distribution has fat tails. I group these three variables together under $n_{norm}$. The remaining numeric variables; *hisei08*, *wealth*, *joyreadp*, *fas* follow distributions with more skewness or clustering, they are grouped separately under $n_{num}$. The variables *sex*, *langreg*, and *pareduc* follow relatively symmetric binomial distributions and are grouped together under $n_{bin}$. Lastly, I create two separate groups for categorical variables with three levels ($n_{cat3}$) and one for categorical variables with four or more levels ($n_{cat4}$). This reduces the sequencing search space from $n!$ to $\prod(n_{norm}!,n_{bin}!,n_{num}!,n_{cat3}!,n_{cat4}!)$, or from 1.3 trillion combinations to $\prod(3!,3!,4!,2!,3!)$; a more manageable 10,368 combinations of synthesis visit sequences. \par

```{r vseq, eval=F}
#plot sequence grid check (run find_sequence.R first)
setwd("H:/IPSDS/Master thesis/Data")
load("test_vseq_ext.RData")
 test.visit <- test.visit %>% mutate(mean_pMSE = (S_pMSE.cc+S_pMSE.cl+S_pMSE.pc+S_pMSE.pl)/4)
 test.visit <- test.visit %>% mutate(mean_pMSE.c = (S_pMSE.cc+S_pMSE.pc)/2)
 test.visit <- test.visit %>% mutate(mean_pMSE.l = (S_pMSE.cl+S_pMSE.pl)/2)
 test.visit <- test.visit %>% mutate(f_pMSE.l = case_when(
   mean_pMSE.l < 1 ~ "1: pMSE < 1", 
   mean_pMSE.l >= 1 & mean_pMSE.l <= 1.5 ~ "2: 1 < pMSE < 1.5",
   mean_pMSE.l > 1.5 & mean_pMSE.l <= 2 ~ "3: 1.5 < pMSE < 2",
   mean_pMSE.l > 2 & mean_pMSE.l <= 2.5 ~ "4: 2 < pMSE < 2.5",
   mean_pMSE.l > 2.5 & mean_pMSE.l < 3 ~ "5: 2.5 < pMSE < 3",
   mean_pMSE.l > 3  ~ "6: pMSE > 3" 
 ))
 p <- 
   ggplot(test.visit, aes(x=S_pMSE.cc, y=S_pMSE.pc,shape=factor(f_pMSE.l))) +
   geom_point() +
   ylim(0.5,7) +
   xlim(0.5,3.5) +
   ggtitle(label="") +
   ylab("CART (S_pMSE) + Parametric (Synthesis)") +
   xlab("CART (S_pMSE) + CART (Synthesis)") +
   guides(shape=guide_legend(title="Average S_pMSE (Logit)")) +
   gghighlight(mean_pMSE < 1.39, label_key = Sequence, label_params = list(size = 2)) 
  #save
    png(file="H:/IPSDS/Master thesis/Pictures/vseq2.png",width=1400, height=900, res=150)
    p
    dev.off()
```

```{r vseq.fig, out.width="95%", fig.cap="Propensity mean-squared error per synthesis visit sequence."}
knitr::include_graphics("H:/IPSDS/Master thesis/Pictures/vseq2.png")
```

To find optimal synthesis sequences, I calculate and compare the standardized propensity mean-squared error ($S\_pMSE$) [Woo et al. -@woo2009; Snoke et al. -@snoke2017]. For each synthesis sequence, I construct two synthetic data sets with *synthpop* [Nowok et al. -@nowok2016]; one using CART and one using parametric models for data synthesis. For each variable, the parametric synthesis models use all remaining variables as predictors with no interactions. To compare the validity of the synthesis, I calculate the  $S\_pMSE$ using both CART and Logit propensity score models. Figure 3 plots the $S\_pMSE$ for each synthesis sequence. The axes show the $S\_pMSE$ calculated with CART propensity score models for both sets of synthetic data, one generated solely by CART models, and one solely generated by parametric models. To make sure that the low $S\_pMSE$ scores calculated by the CART propensity score models are somewhat robust, I also calculate the average $S\_pMSE$ by use of a Logit propensity score model. The six points highlighted in Figure 3 have an average $S\_pMSE$ smaller than 1.5 (as calculated by both the CART and Logit propensity score models). I will use these six synthesis sequences to further fine-tune the synthesis models for each individual variable in the following section. The numbers in these sequences corresponds to the order of the variable list in Table 1. \par  

## Modelling
For each of the six best variable visit sequences, Figure 4 plots the $S\_pMSE$ calculated with CART models per variable for both the CART and parametric data syntheses. Highlighted are sequences and models where the $S\_pMSE$ is below 0.8. \par

```{r model.per, eval=F}
m <- 1
set.seed(6541)
vseq <- list(
  c(12, 3, 4, 2, 7, 1, 9, 6, 10, 11, 14, 8, 15, 13, 5),#1
  c(12, 3, 4, 2, 7, 1, 6, 11, 10, 9, 14, 8, 13, 15, 5),#2
  c(12, 4, 3, 7, 1, 2, 10, 6, 11, 9, 14, 8, 13, 15, 5),#3
  c(4, 12, 3, 7, 1, 2, 10, 11, 6, 9, 8, 14, 15, 5, 13),#4
  c(4, 12, 3, 7, 1, 2, 6, 10, 11, 9, 8, 14, 15, 5, 13),#5
  c(4, 12, 3, 1, 7, 2, 9, 10, 11, 6, 8, 14, 5, 13, 15) #6
  )
c <- 0
for (v in c(1,2,5,7,8,13,14,15)){
  obs[v] <- as.factor(obs[,v])
}
rm(v)
for (v in vseq){
  c <- c+1
  syn.cart <- paste0("syn_cart",c)
  syn.par <- paste0("syn_par",c)
  assign(syn.cart, syn(obs,m=m,method="cart",visit.sequence=v))
  assign(syn.par, syn(obs,m=m,method="parametric",visit.sequence=v))
}  
varlist <- factor(c(
  "sex","langreg","wlem","marks","ls_req","hisei08","pareduc","immig",
  "wealth","joyreadp","fas","inccap","aspmf","aspideal","us_enroll"),
  levels = c("sex","langreg","wlem","marks","ls_req","hisei08","pareduc","immig",
  "wealth","joyreadp","fas","inccap","aspmf","aspideal","us_enroll"),ordered=T)
comp_tab <- data.frame(
  var=rep(varlist,12),
  visit.seq=c(rep(1,30),rep(2,30),rep(3,30),rep(4,30),rep(5,30),rep(6,30)),
  syn.method=rep(c(rep("CART",15),rep("Parametric",15)),6),
  S_pMSE = c(
    compare.synds(syn_cart1,obs)$tab.utility[,2],
    compare.synds(syn_par1,obs)$tab.utility[,2],
    compare.synds(syn_cart2,obs)$tab.utility[,2],
    compare.synds(syn_par2,obs)$tab.utility[,2],
    compare.synds(syn_cart3,obs)$tab.utility[,2],
    compare.synds(syn_par3,obs)$tab.utility[,2],
    compare.synds(syn_cart4,obs)$tab.utility[,2],
    compare.synds(syn_par4,obs)$tab.utility[,2],
    compare.synds(syn_cart5,obs)$tab.utility[,2],
    compare.synds(syn_par5,obs)$tab.utility[,2],
    compare.synds(syn_cart6,obs)$tab.utility[,2],
    compare.synds(syn_par6,obs)$tab.utility[,2]
  ))
u <- comp_tab %>% 
  ggplot(aes(x=reorder_within(factor(visit.seq),S_pMSE,list(var,syn.method)), y=S_pMSE, fill=syn.method))+
  geom_col(width=0.8, position = "dodge") + 
  coord_flip() + 
  xlab("Visit sequence") +
  ylab("S_pMSE") + 
  guides(fill=guide_legend(title="Synthesis Method")) +
  theme(axis.text=element_text(size=8),
        legend.position="top",
        legend.title = element_blank(),
        legend.text = element_text(size=9),
        axis.title = element_text(size=9),
        strip.text.x = element_text(size=9)
        ) +  
  scale_x_reordered() + 
  facet_wrap(~ var, scales = "free",nrow=3,ncol=5) + 
  gghighlight(S_pMSE < 0.8, use_group_by = F, calculate_per_facet = T, unhighlighted_params = list(fill = NULL, alpha = 0.3))
png(file="H:/IPSDS/Master thesis/Pictures/synmeth2.png",width=1400, height=1000, res=150)
u
dev.off()
```

```{r model.fig, out.width="95%", fig.cap="Variable utility with CART or Parametric synthesis for 6 best performing visit sequences"}
knitr::include_graphics("H:/IPSDS/Master thesis/Pictures/synmeth2.png")
```

```{r joyreadtest, eval=F}
x <- obs[!is.na(obs$wlem) & 
           !is.na(obs$marks) & 
           !is.na(obs$hisei08) & 
           !is.na(obs$wealth) & 
           !is.na(obs$fas) &
           !is.na(obs$inccap),]
for (v in c(1,2,5,7,8,13,14,15)){
  x[v] <- as.factor(x[,v])
}
reg1 <- lm(joyreadp ~ .*. + 
            poly(wlem,3) + 
            poly(marks,3) + 
            poly(hisei08, 3) + 
            poly(wealth, 3) + 
            poly(fas, 3) + 
            poly(inccap, 3), data=x)
reg2 <- lm(joyreadp ~ ., data=x)
rsq1 <- summary(reg1)$adj.r.squa
rsq2 <- summary(reg2)$adj.r.squa
```

Each table facet in Figure 4 contains the utility in $S\_pMSE$ for one of the 15 variables. The vertical axis list 12 synthesis runs (the best 6 sequences for both CART- and Parametric synthesis) and orders them by their $S\_pMSE$ to get a better idea if some variables consistently perform better with either CART or Parametric data synthesis. Most of the variables show a mixed picture where, depending on the sequencing, either CART or parametric modeling leads to the lowest $S\_pMSE$. However, there are some variables for which it is relatively clear that CART synthesis outperforms parametric modeling. It should be noted that this comparison is only between CART and a relatively straightforward parametric model that includes all remaining variables as main effects without any interactions or higher order polynomials. Obviously, the parametric modelling could be improved to better model the data generating process. To check whether a more complex model could improve the fit, I run a linear model for *joyreadp* that includes a range of interaction terms as well as second and third order polynomials for all continuous variables. Compared to the more straightforward model used in Figure 4 this approach does increase the model fit measured in explained variance, but the increase is marginal and the overall explained variance remains fairly low at 0.146 (McFadden adjusted R-squared). It therefore seems reasonable to assume that for the three variables *ls_req* (lower-secondary school type), *joyreadp* (parental reading interest), and *fas* (family affluence), using CART models for synthesis would lead to an overall better data utility. \par

```{r model.pick, eval=F}
#Run find_model.R fist 
load("H:/IPSDS/Master thesis/Data/test_model_redux4.RData")
test.model$Model <- str_replace_all(test.model$Model, "parametric", "P")
test.model$Model <- str_replace_all(test.model$Model, "cart", "C")
test.model$Model <- str_replace_all(test.model$Model, "sample", "s")
n <- ggplot(test.model, aes(x=S_pMSE.c, y=S_pMSE.l, shape=Sequence)) +
   geom_point() +
   ylim(0,3) +
   xlim(1,5) +
   scale_shape_manual(values=c(16,17,15,7,3,4,2,1))+
   ggtitle(label="") +
   ylab("S_pMSE (Logit)") +
   xlab("S_pMSE (CART)") +
   gghighlight(S_pMSE.l < 1, S_pMSE.c < 2.5, label_key = Model, label_params = list(size = 3)) +
   theme(strip.text.x = element_text(size=6),
         axis.text=element_text(size=6),
         axis.title = element_text(size=8),
         legend.title = element_text(size=10),
         legend.text = element_text(size=8))
    png(file="H:/IPSDS/Master thesis/Pictures/model2.png",width=1200, height=700, res=150)
    n
    dev.off()
```

```{r model.pick.fig, out.width="95%", fig.cap="CART / parametric modeling per visit sequence."}
knitr::include_graphics("H:/IPSDS/Master thesis/Pictures/model2.png")
```

For the six visit sequences, I will test which combination of modelling will lead to the smallest $S\_pMSE$. For the variables *sex, ls_req, hisei08, wealth, joyreadp,* and *fas* I will only consider CART modelling, for the variables *langreg, wlem, marks, pareduc, inccap, aspmf,* and *us_enroll* I will test both CART and parametric models, and for *immig,* and *aspideal* I will look at parametric models exclusively. If only half of the 15 variables can vary between models, this gives a total of $\prod_{n=1}^{7}(2!)$ or 128 different combinations per visit sequence. Figure 5 shows the best performing modelling and variable visit sequences, with the capital letters C and P standing for "CART" and "parametric", and lower case s standing for "sample", i.e. the first variable in the synthesis sequence. I will use these modelling parameters to run the final data synthesis. \par 

```{r modelfit, eval = F}
d <- obs 
d[is.na(d$pareduc),"pareduc"] <- 3 
d[is.na(d$ls_req),"ls_req"] <- 5
d[is.na(d$immig),"immig"] <- 4
d[is.na(d$aspmf),"aspmf"] <- 5
d[is.na(d$aspideal),"aspideal"] <- 3
for (v in c(1,2,5,7,8,13,14,15)){
  d[v] <- as.factor(d[,v])
}
x <- multinom(pareduc ~ inccap + marks + wlem,
              data=d)
y <- multinom(immig ~ .-us_enroll-ls_req-aspmf,
              data=d)
z <- multinom(aspideal ~ .-immig-us_enroll-ls_req-aspmf,
              data=d)
u <- multinom(us_enroll ~ .-ls_req-aspmf,
              data=d)
png(file="H:/IPSDS/Master thesis/Pictures/resid.png",width=900, height=900, res=150)
par(mfrow=c(2,2))
plot(density(resid(x, type='deviance')),col="red",xlab="",main="Education parents (pareduc)",lwd=2,cex.main=0.9)
plot(density(resid(y, type='deviance')),col="red",xlab="",ylab="",main="Immigration status (immig)",lwd=2,cex.main=0.9)
plot(density(resid(z, type='deviance')),col="red",main="Education aspirations (aspideal)",xlab="Residuals",lwd=2,cex.main=0.9)
plot(density(resid(u, type='deviance')),col="red",main="Enrollment upper-sec. (us_enroll)",ylab="",xlab="Residuals",lwd=2,cex.main=0.9)
dev.off()
```

```{r modelfit.fig, out.width="60%", fig.align='center', fig.cap="Deviance residuals for multinominal logit synthesis models."}
knitr::include_graphics("H:/IPSDS/Master thesis/Pictures/resid.png")
```

Since missing values will be treated as separate categories, all variables that will be synthesized by fitting parametric models (*pareduc*, *immig*, *aspideal*, and *us_enroll*) will use a multinominal logit model to generate synthetic values. The variable *pareduc* is the fourth synthesized variable and only has three explanatory variables (*inccap*,*marks*, and *wlem*). The variables *aspideal*, *immig*, and *us_enroll* are all synthesized later in the sequence (11th, 12th, and 13th place, respectively) and contain all remaining variables as predictors minus the last two variables in the synthesis sequence: *ls_req* and *aspmf*. To see how well these models describe the data, Figure 6 shows the density plots for the regression residuals for each of the four variables. The regression models for *immig*, *aspideal*, and *us_enroll* seem to fit the data quite well since the residuals mostly spike around zero. The residuals for *pareduc* show that there are some more discrepancies between the observed and estimated values in the regression model but the residuals are generally still small. \par 

```{r synthesis, eval=F}
set.seed(6541)
m <- 5
vseq <- c(12,4,3,7,2,1,9,11,10,6,14,8,15,5,13) 
model <- c("cart", #1 sex
            "cart", #2 langreg
            "cart", #3 wlem
            "cart", #4 marks
            "cart", #5 ls_req
            "cart", #6 hisei08
            "parametric", #7 pareduc
            "parametric", #8 immig
            "cart", #9 wealth
            "cart", #10 joyreadp
            "cart", #11 fas
            "sample", #12 inccap
            "cart", #13 aspmf
            "parametric", #14 aspideal
            "parametric" #15 us_enroll
            )
rm(v)
syn <- syn(obs,
        m=m,
        method=model,
        visit.sequence=vseq
)
setwd("H:/IPSDS/Master thesis/Data")
save(syn, file="syn2.RData")
```

```{r loadsyn}
load("H:/IPSDS/Master thesis/Data/syn2.Rdata")
```

```{r compare.synth, eval=F}
comp <- compare.synds(syn,obs,
                      nrow=5,
                      ncol=3,
                      breaks = 10,
                      stat = "percents",
                      rel.size.x = 0.5
                      )
```

## Data utility 

```{r comp.plot, eval=F}
comp.data <- as.data.frame(comp$plots[[1]])
levels(comp.data$Value)[levels(comp.data$Value)=='miss.NA'] <- NA
comp.data$Variable <- sub(":.+", "", comp.data$Variable)
comp.data$Variable <- factor(comp.data$Variable, levels = c("sex","langreg","wlem","marks","ls_req","hisei08","pareduc","immig","wealth","joyreadp","fas","inccap","aspmf","aspideal","us_enroll"),ordered=T)
p <- ggplot(comp.data, aes(x=Value, y=Percent,fill=Data)) +
  geom_col(position = "dodge") +
  facet_wrap(~ Variable, scales = "free", nrow=4, ncol=4, shrink=F) +
  theme_bw() + 
  theme(axis.text=element_text(size=5),
        legend.position="top",
        legend.title = element_blank(),
        legend.text = element_text(size=8),
        axis.title = element_text(size=8),
        strip.text.x = element_text(size=6)
        ) 
    png(file="H:/IPSDS/Master thesis/Pictures/comp.png",width=1200, height=900, res=150)
    p
    dev.off()
```

```{r comp.fig, out.width="100%", fig.cap="Marginal distributions for synthetic and observed data."}
knitr::include_graphics("H:/IPSDS/Master thesis/Pictures/comp.png")
```

With the sequence and modelling parameters set in section 4.2, I generate five synthetic data sets using *synthpop* [Nowok et al. -@nowok2016]. As mentioned previously, missing values in the observed data are retained in the synthetic data. Figure 7 plots the marginal distributions of each of the 15 variables in the synthetic and the observed data. The percentages for the synthetic data are based on the pooled sets of generated synthetic data. In the univariate comparison, the synthetic data generally follow the observed data fairly closely. Some more notable differences exist between the synthetic and observed data for the variables *joyreadp* and *fas*. The data synthesis for these variables could likely be improved by adding additional information to the data set or tweaking the data and modelling. Both the marginal distributions for *joyreadp* and *fas* are somewhat skewed. Performing some kind of transformation (e.g. log or square-root transformations) on the original data and trying out different modelling approaches might improve the utility of the synthetic data.\par

To look at the multivariate validity of the data, I run a set of regression models on the synthetic data and the observed data and compare the overlap of the confidence intervals. For each variable, the regression model contains all remaining variables as predictors without any interactions or higher order terms. The variables *sex*, *langreg*, and *pareduc* are estimated by fitting a Logit model; the variables *wlem*, *marks*, *hisei08*, *wealth*, *joyreadp*, *fas*, and *inccap* by fitting a linear model; and the variables *ls_req*, *immig*, *aspmf*, *aspideal*, and *us_enroll* by fitting a multinomial logit model. \par

```{r ci.overlap, message=F, include=F,eval=F}
m <- compare(glm.synds(sex~.,binomial(link="logit"),syn),obs,population.inference = T,incomplete=T, plot='coef')$ci.overlap
ci <- data.frame(y="sex",x=rownames(m),ci.overlap=m[,1])
m <- compare(glm.synds(langreg~.,binomial(link="logit"),syn),obs,population.inference = T,incomplete=T)$ci.overlap
t <- data.frame(y="langreg",x=rownames(m),ci.overlap=m[,1])
ci <- rbind(ci,t)
m <- compare(glm.synds(wlem~ .,gaussian,syn),obs,population.inference = T,incomplete=T)$ci.overlap
t <- data.frame(y="wlem",x=rownames(m),ci.overlap=m[,1])
ci <- rbind(ci,t)
m <- compare(glm.synds(marks~ .,gaussian,syn),obs,population.inference = T,incomplete=T)$ci.overlap
t <- data.frame(y="marks",x=rownames(m),ci.overlap=m[,1])
ci <- rbind(ci,t)
m <- compare(multinom.synds(ls_req~.,syn),obs,population.inference = T,incomplete=T)$ci.overlap
t <- data.frame(y="ls_req",x=rownames(m),ci.overlap=m[,1])
ci <- rbind(ci,t)
m <- compare(glm.synds(hisei08~ .,gaussian,syn),obs,population.inference = T,incomplete=T)$ci.overlap
t <- data.frame(y="hisei08",x=rownames(m),ci.overlap=m[,1])
ci <- rbind(ci,t)
m <- compare(glm.synds(pareduc~.,binomial(link="logit"),syn),obs,population.inference = T,incomplete=T)$ci.overlap
t <- data.frame(y="pareduc",x=rownames(m),ci.overlap=m[,1])
ci <- rbind(ci,t)
m <- compare(multinom.synds(immig~.,syn),obs,population.inference = T,incomplete=T)$ci.overlap
t <- data.frame(y="immig",x=rownames(m),ci.overlap=m[,1])
ci <- rbind(ci,t)
m <- compare(glm.synds(wealth~ .,gaussian,syn),obs,population.inference = T,incomplete=T)$ci.overlap
t <- data.frame(y="wealth",x=rownames(m),ci.overlap=m[,1])
ci <- rbind(ci,t)
m <- compare(glm.synds(joyreadp~ .,gaussian,syn),obs,population.inference = T,incomplete=T)$ci.overlap
t <- data.frame(y="joyreadp",x=rownames(m),ci.overlap=m[,1])
ci <- rbind(ci,t)
m <- compare(glm.synds(fas~ .,gaussian,syn),obs,population.inference = T,incomplete=T)$ci.overlap
t <- data.frame(y="fas",x=rownames(m),ci.overlap=m[,1])
ci <- rbind(ci,t)
m <- compare(glm.synds(inccap~ .,gaussian,syn),obs,population.inference = T,incomplete=T)$ci.overlap
t <- data.frame(y="inccap",x=rownames(m),ci.overlap=m[,1])
ci <- rbind(ci,t)
m <-compare(multinom.synds(aspmf~.,syn),obs,population.inference = T,incomplete=T)$ci.overlap
t <- data.frame(y="aspmf",x=rownames(m),ci.overlap=m[,1])
ci <- rbind(ci,t)
m <-compare(multinom.synds(aspideal~.,syn),obs,population.inference = T,incomplete=T)$ci.overlap
t <- data.frame(y="aspideal",x=rownames(m),ci.overlap=m[,1])
ci <- rbind(ci,t)
m <-compare(multinom.synds(us_enroll~.,syn),obs,population.inference = T,incomplete=T)$ci.overlap
t <- data.frame(y="us_enroll",x=rownames(m),ci.overlap=m[,1])
ci <- rbind(ci,t)
ci <- ci[ci$x!="(Intercept)",]
```

```{r ci.plot,echo=F, eval=F}
ci$y <- factor(ci$y, levels = c("sex","langreg","wlem","marks","ls_req","hisei08","pareduc","immig",
             "wealth","joyreadp","fas","inccap","aspmf","aspideal","us_enroll"),ordered=T)
c <- ggplot(ci, aes(x=y,y=ci.overlap)) + 
  geom_boxplot() + 
  ylab("95% CI overlap") +
  xlab("") + 
  scale_x_reordered() + 
  theme(axis.text=element_text(size=7)) + 
  ylim(0.2,1)
    png(file="H:/IPSDS/Master thesis/Pictures/cioverlap2.png",width=1000, height=600, res=150)
    c
    dev.off()
```

```{r ci.fig, out.width="90%", fig.align='center', fig.cap="Box-plot for confidence interval overlaps per outcome variable"}
knitr::include_graphics("H:/IPSDS/Master thesis/Pictures/cioverlap2.png")
```

Figure 8 plots the distribution of the CI overlaps for all the regression coefficients in each of the 15 regression models. The median CI overlap generally lies around or above the 75 percent line. However, there are some variables for which a significant number of coefficients overlap relatively poorly. Specifically, the tail end of the CI-overlap distributions of *hisei08* and *aspideal* are rather long, indicating that, for these models, a larger share of the point estimate's CI fitted on the synthetic data deviate from those fitted on the original data. \par

## Disclosure risk 
The appropriate measure of disclosure risk of disseminating synthetic data based on linked data from the TREE panel and the LABB register is measuring the added risk of disclosing information about the true value of educational outcomes, in particular for those who explicitly or implicitly refused to disclose this information in the TREE panel survey. For the calculation of the *Targeted Correct Attribution Probability* (TCAP), the target variable is therefore defined as the upper-secondary enrollment in 2017. As key variables, it seems reasonable to pick demographic and background information that might be available in other publicly available data sources. Therefore vectors $K$ and $T$ are defined as: \par

\hfil \begin{equation}K=\{sex, langreg, ls\_req, pareduc, immig\}\end{equation} \par

\hfil \begin{equation}T=\{us\_enroll\}\end{equation} \par

Figure 9 plots the TCAP scores for each of the five synthesis runs alongside the percentage of rows in the data sets that have a WEAP score of one (i.e. synthetic records for which the key variables uniquely identify the target variables). From the figure, it is clear that the synthetic data generally have a low risk of disclosure. In three of the five runs, no rows with WEAP=1 have matching rows in the original data, making it very unlikely that an attacker could infer information on educational outcomes from the synthetic data without significant uncertainty. In synthetic data sets two and five, there are a few rows that drive up the TCAP score and, even though the overall score is low, a possibility exists for an attacker to correctly identify educational outcomes for at least some individuals in the original data. This risk is still relatively small because, without prior knowledge, an attacker would not know which rows are at risk. To reduce the risk to an absolute minimum, however, only synthetic data sets with a TCAP score of zero could be published. \par 

```{r tctab, eval=F}
tctab <- data.frame()
source("H:/IPSDS/Master thesis/tcap.R")
for (i in 1:length(syn$syn[])){
  y <- tcap(syn$syn[[i]],obs,c(1,2,5,7,8),c(15))
  y$run <- i 
  #x <- data.frame(run=i, rows=nrow(y), t=mean(y$tcap))
  tctab <- rbind(y, tctab)
}
setwd("H:/IPSDS/Master thesis/Data")
save(tctab, file="tctab2.RData")
```

```{r tcplot, eval=F}
setwd("H:/IPSDS/Master thesis/Data")
load("tctab.RData")
tctab$rows <- tctab$rows/8429*100
b <- tctab %>% 
  ggplot() +
  geom_col(aes(x=run, y=rows),fill="grey",alpha=0.5,width=0.5,color="black") +
  geom_point(aes(x=run,y=t, color="TCAP score"), size=3) + 
  xlab("Synthetic data set") + 
  ylab("% rows (WEAP = 1)") + 
  theme_minimal() +
  theme(axis.title=element_text(size=7),
        axis.text=element_text(size=6)) +  
  scale_color_manual(name='',breaks=c('TCAP score'),values=c('TCAP score'='red'))
png(file="H:/IPSDS/Master thesis/Pictures/tcap.png",width=800, height=400, res=150)
b
dev.off() 
```

```{r tcfig,out.height="40%",fig.align="center",fig.cap="TCAP score per generated synthetic data set."}
knitr::include_graphics("H:/IPSDS/Master thesis/Pictures/tcap.png")
```

# Conclusion
The aim of this paper is to illustrate the feasibility of generating synthetic data based on linked panel survey and register data. The main motivation for this exercise is the knowledge that panel attrition can lead to biases in survey statistics. Ideally, panel survey data could be supplemented with data taken from official registries, wherever available. However, with participation in panel surveys usually comes the explicit consent to have this information (anonymously) published. Nonresponse, in that way, can be seen as an explicit or implicit refusal for the disclosure of such information, even in anonymous form. Therefore, even if information from registries could be used to complement panel survey data, it would neither be ethical nor legal, in most cases, to publish this data without additional layers of disclosure control. \par

In this paper, I have shown that such an approach is feasible. Generating synthetic data based on linked panel survey and register data can produce statistical products that have a reasonable level of utility without adding much risk of disclosure if they were to be disseminated. Ideally, such a product could be used in a similar way to the synthetic versions of the EU-SILC published by Eurostat. If publicly available, researchers could develop their analyses on the synthetic version of the linked TREE and LABB data and check their results on the original data in a more controlled setting. \par  

As a note, the synthesis models used in this paper are simple. With more model fine-tuning, I believe that the utility of a final statistical product could be further improved. In addition, I have selected the synthesis variables to best fit the modelling of educational outcomes with an overall straightforward data structure. The selection of variables and the complexity of the data could be expended to facilitate a wider range of analyses. \newpage 

# References {-}
\setstretch{1}
::: {#refs}
:::
